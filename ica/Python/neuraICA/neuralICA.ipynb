{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import tensorflow as  tf\n",
    "\n",
    "import scipy.io as scio\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_standard(x):\n",
    "    return np.exp(-0.5*np.square(x))/(np.sqrt(2*np.pi))\n",
    "\n",
    "def scale(X,center=True,scale=True):\n",
    "    if(center==True):\n",
    "        X =X -np.mean(X,axis=0)\n",
    "    if(scale==True):\n",
    "        X =X/np.std(X,axis=0)\n",
    "    return X\n",
    "    \n",
    "def whiten(X):\n",
    "    [N,p] =X.shape\n",
    "    cov =(X.T@X)/N\n",
    "    [eig_vals,eig_vecs] =np.linalg.eig(cov)\n",
    "    P =(np.diag(1.0/np.sqrt(eig_vals)))@eig_vecs.T\n",
    "    return [P,(P@X.T).T]\n",
    "\n",
    "def group(y,B=500):\n",
    "    N =y.shape[0]\n",
    "    y =np.sort(y)\n",
    "    y_min =y[0]\n",
    "    y_max =y[-1]\n",
    "    freqs =np.zeros((B,),dtype=float)\n",
    "    ys    =np.zeros((B,),dtype=float)\n",
    "    gaps =(y_max-y_min)/(B-1)\n",
    "    left =y_min -0.5*gaps\n",
    "    ys =y_min+gaps*np.arange(B)\n",
    "    index =0\n",
    "    for i in range(N):\n",
    "        index =int(np.floor((y[i]-left)/gaps))\n",
    "        freqs[index] = freqs[index]+1.0\n",
    "    freqs =freqs/gaps\n",
    "    return [ys,freqs]\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "def Amari_metric(A0,A):\n",
    "    R =np.abs(A0@np.linalg.inv(A))\n",
    "    m =A0.shape[0]\n",
    "    row_max =np.max(R,axis=1)\n",
    "    col_max =np.max(R,axis=0)\n",
    "    return -1.0+(np.sum(np.sum(R,axis=1)/row_max)+np.sum(np.sum(R,axis=0)/col_max))/(2*m)\n",
    "    \n",
    "def mixmat(m):\n",
    "    A =np.random.normal(size=(m,m))\n",
    "    [u,s,vh] =np.linalg.svd(A,full_matrices=True)\n",
    "    d =np.sort(np.random.uniform(low=0.0,high=1.0,size=(m,)))+1.0\n",
    "    print('condition ',d[-1]/d[0])\n",
    "    A =u@vh.T@np.diag(d)\n",
    "    return A\n",
    "\n",
    "def orth(W):\n",
    "    [u,s,vh] =np.linalg.svd(W,full_matrices=True)\n",
    "    W =u@vh\n",
    "    return W\n",
    "\n",
    "\n",
    "def mdi_loss(y_true,y_pred):\n",
    "    #怎么不是在最小化，而是在增大\n",
    "    norm_val =K.exp(y_pred)*y_true\n",
    "    #return (K.log(K.mean(norm_val,axis=-1))-(K.mean(norm_val*y_pred,axis=-1)/K.mean(norm_val,axis=-1)))\n",
    "    return ((K.log(K.mean(norm_val,axis=-1))-(K.mean(y_pred,axis=-1)/(K.mean(norm_val,axis=-1)+K.epsilon()))))\n",
    "def mdi_loss2(y_true,y_pred):\n",
    "    norm_val =K.exp(y_pred)*y_true\n",
    "    return K.mean(norm_val,axis=-1)-K.mean(y_pred,axis=-1)\n",
    "\n",
    "\n",
    "def mdi_loss3(y_true,y_pred):\n",
    "    norm_val =K.exp(y_pred)*y_true\n",
    "    return K.mean(norm_val,axis=-1)*K.log(K.mean(norm_val,axis=-1))-K.mean(y_pred,axis=-1)\n",
    "\n",
    "def mdi_loss4(y_true,y_pred):\n",
    "    #很快收敛但是十分不稳定。\n",
    "    return K.mean(K.exp(y_pred),axis=-1)-K.mean(y_true*y_pred,axis=-1)\n",
    "\n",
    "def mdi_loss5(y_true,y_pred):\n",
    "    #很快收敛但是十分不稳定。\n",
    "    return K.sum(y_pred,axis=-1)-K.sum(y_true*K.log(y_pred),axis=-1)\n",
    "\n",
    "def negentropy(y_true,y_pred):\n",
    "    return K.mean(y_pred,axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r\"./data/dists.mat\"\n",
    "data = scio.loadmat(path)[\"dists\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S shape  (1024, 2)\n",
      "condition  1.17251784136\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89c60929e8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89c5e241d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m  =2\n",
    "S  =data[[1,1],[3,4],:].T\n",
    "#s1 =np.random.binomial(100,0.33,1024)[:,np.newaxis]\n",
    "#s2 =np.random.binomial(250,0.55,1024)[:,np.newaxis]\n",
    "#S =np.c_[s1,s2]\n",
    "print('S shape ',S.shape)\n",
    "A0 =mixmat(2)\n",
    "#A0 =np.array([[0.4,0.6],[0.2,0.7]],dtype=float)\n",
    "X  =(A0@S.T).T\n",
    "\n",
    "plt.subplots(figsize=(12,6))\n",
    "for i in range(m):\n",
    "    plt.subplot(m,1,i+1)\n",
    "    plt.hist(S[:,i])\n",
    "\n",
    "\n",
    "plt.subplots(figsize=(12,6))\n",
    "for i in range(m):\n",
    "    plt.subplot(m,1,i+1)\n",
    "    plt.hist(X[:,i])\n",
    "\n",
    "X  =scale(X,center=True,scale=False)\n",
    "[P,X] =whiten(X)\n",
    "target =np.linalg.inv(P@A0)\n",
    "W0 =np.random.normal(size=(2,2))\n",
    "W0 =orth(W0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_session = K.get_session()\n",
    "tf_session.run(tf.global_variables_initializer())\n",
    "y =K.variable([-0.5,2.0], dtype='float32', name=\"y\") \n",
    "w =K.variable(np.ones((1024)), dtype='float32', name=\"w\")\n",
    "print(\"y \",K.eval(y))\n",
    "print(\"w \",K.eval(w))\n",
    "res=-1.0*K.square(y)\n",
    "print(K.eval(K.gradients(res,[y])[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralICA(X,W0,M=1024,maxiters=20,epochs=400,optimizer='rmsprop',tol=1e-7,activation='sigmoid',lbd=0.05):\n",
    "    [N,p] =X.shape\n",
    "    batch_size =N\n",
    "    m =p\n",
    "    models =[]\n",
    "    inputs =[]\n",
    "    outputs =[]\n",
    "    loss     =np.zeros((maxiters,m),dtype=float)\n",
    "    metrices =np.zeros((maxiters,m),dtype=float)\n",
    "    y_pred =np.zeros((N,m),dtype=float)\n",
    "    tol =1e-7\n",
    "    W=W0\n",
    "    W_last =np.ones(W0.shape,dtype=float)\n",
    "    for i in range(m):\n",
    "        input_tensor =Input(shape=(1,),dtype='float32')\n",
    "        x =layers.Dense(M,activation=activation,kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(input_tensor)\n",
    "        #x =layers.Dense(50,activation='sigmoid',kernel_initializer='random_uniform',\n",
    "        #            bias_initializer='zeros',kernel_regularizer=regularizers.l1(1))(x)\n",
    "        output_tensor =layers.Dense(1,activation=\"linear\",kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros',kernel_regularizer=regularizers.l2(lbd))(x)\n",
    "        model =Model(input_tensor,output_tensor)\n",
    "        model.compile(optimizer=optimizer,loss=mdi_loss2,metrics=[negentropy])\n",
    "        models.append(model)\n",
    "        inputs.append(input_tensor)\n",
    "        outputs.append(output_tensor)\n",
    "\n",
    "    for i in range(maxiters):\n",
    "        print('iteration ',i,' begin')\n",
    "        X_train =(W@X.T).T\n",
    "        y_train =gauss_standard(X_train)\n",
    "        #print('dist ',Amari_metric(W_last,W))\n",
    "        if(Amari_metric(W_last,W)<tol):\n",
    "            break\n",
    "        W_last =W.copy()\n",
    "        for j in range(m):\n",
    "            for k in range(epochs):\n",
    "                #print('epochs ',k)\n",
    "                [loss[i,j],metrices[i,j]] =models[j].train_on_batch(x=X_train[:,j],y=y_train[:,j])\n",
    "                #models[j].fit(X_train[:,j][:,np.newaxis],y_train[:,j][:,np.newaxis],epochs=100,batch_size=1024,verbose=0)\n",
    "                grad  =K.gradients(models[j].output,[models[j].input])[0]\n",
    "                grad2 =K.gradients(grad,[models[j].input])[0]\n",
    "                grads =K.function([models[j].input],[grad,grad2])\n",
    "                #print('grad ',grad)\n",
    "                #print('grad2 ',grad2)\n",
    "                #K.update(models[j].input,X_train[:,j])\n",
    "                [grad_val,grad2_val] =grads([X_train[:,j][:,np.newaxis]])\n",
    "        \n",
    "                #print('norm ',np.linalg.norm(grad_val))\n",
    "                #print('grad_val ',grad_val)\n",
    "                W[j,:] =np.mean(X*grad_val,axis=0)-np.mean(grad2_val)*W[j,:]\n",
    "                #if(j>0):\n",
    "                #      W[j,:] =W[j,:]-(np.sum(W[j,:]*W[j-1,:]))*W[j-1,:]\n",
    "                #W[j,:] =W[j,:]/np.linalg.norm(W[j,:])\n",
    "               #print('before W ',W)\n",
    "        W =orth(W)\n",
    "        #if(i<10):\n",
    "        #    print('W_last ',W_last)\n",
    "        #    print('W ',W )\n",
    "        #print('epoch ',i,' Amari metric: ',Amari_metric(target,W))\n",
    "        \n",
    "        print('iteration ',i,' loss ',loss[i,:],' negentropy ',metrices[i,:])\n",
    "    return [W,metrices]\n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss=mdi_loss,metrics=[negentropy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amari metric  0.142456072966\n",
      "dist  0.418396952833\n",
      "epoch  0  Amari metric:  0.93481166614\n",
      "loss  [ -1.57852710e+09  -1.06142675e+06]\n",
      "metrices  [ 15.14108276   9.67463684]\n",
      "dist  0.698813598568\n",
      "epoch  1  Amari metric:  0.817036672799\n",
      "loss  [ -7.86921440e+07  -2.67438926e+04]\n",
      "metrices  [ 15.03038883   8.1757021 ]\n",
      "dist  0.135548343911\n",
      "epoch  2  Amari metric:  0.726329029843\n",
      "loss  [-7751785.5          -15444.62597656]\n",
      "metrices  [ 11.16394806   7.71894932]\n",
      "dist  0.0568421760264\n",
      "epoch  3  Amari metric:  0.682020591891\n",
      "loss  [-2759926.75         -16731.91015625]\n",
      "metrices  [ 10.47092819   7.85259581]\n",
      "dist  0.029591977076\n",
      "epoch  4  Amari metric:  0.672158624942\n",
      "loss  [-1661622.5          -19094.34960938]\n",
      "metrices  [ 9.95923233  7.95991659]\n",
      "dist  0.00675375485571\n",
      "epoch  5  Amari metric:  0.633547442259\n",
      "loss  [-1503711.25         -20478.90039062]\n",
      "metrices  [ 8.92173004  7.96365738]\n",
      "dist  0.0270478696\n",
      "epoch  6  Amari metric:  0.6484127249\n",
      "loss  [-959023.1875    -25463.453125]\n",
      "metrices  [ 9.35277843  8.0416069 ]\n",
      "dist  0.0105247686316\n",
      "epoch  7  Amari metric:  0.61649348107\n",
      "loss  [-1146080.           -23921.42578125]\n",
      "metrices  [ 7.90393066  7.73874474]\n",
      "dist  0.0227781494856\n",
      "epoch  8  Amari metric:  0.633304437785\n",
      "loss  [-808509.3125   -29945.53125]\n",
      "metrices  [ 8.79426003  7.57364368]\n",
      "dist  0.0120771902801\n",
      "epoch  9  Amari metric:  0.611241959381\n",
      "loss  [-983851.1875      -26782.42382812]\n",
      "metrices  [ 7.13421059  7.3643074 ]\n",
      "dist  0.0158880495646\n",
      "epoch  10  Amari metric:  0.62279796522\n",
      "loss  [-778933.8125      -31576.62304688]\n",
      "metrices  [ 8.16372204  7.30856037]\n",
      "dist  0.00836074538733\n",
      "epoch  11  Amari metric:  0.608215855052\n",
      "loss  [-902728.       -28646.71875]\n",
      "metrices  [ 6.43206453  7.25788403]\n",
      "dist  0.0105645848378\n",
      "epoch  12  Amari metric:  0.608977409635\n",
      "loss  [-752872.8125     -32423.8046875]\n",
      "metrices  [ 7.6859498   7.24153614]\n",
      "dist  0.000555131924727\n",
      "epoch  13  Amari metric:  0.599430599064\n",
      "loss  [-773368.1875      -31997.61132812]\n",
      "metrices  [ 5.95188665  7.19622278]\n",
      "dist  0.00698645548791\n",
      "epoch  14  Amari metric:  0.575181649322\n",
      "loss  [-716263.1875      -35058.35546875]\n",
      "metrices  [ 7.28941393  7.17459679]\n",
      "dist  0.0180136919817\n",
      "epoch  15  Amari metric:  0.564270911804\n",
      "loss  [-578034.4375      -43777.93359375]\n",
      "metrices  [ 5.73386717  7.04069042]\n",
      "dist  0.00822929963567\n",
      "epoch  16  Amari metric:  0.494662641124\n",
      "loss  [-520575.71875   -49720.265625]\n",
      "metrices  [ 7.2795229  6.9408164]\n",
      "dist  0.0543711894383\n",
      "epoch  17  Amari metric:  0.455083442953\n",
      "loss  [-319991.9375    -120052.7109375]\n",
      "metrices  [ 7.71769428  6.67844868]\n",
      "dist  0.0322824251474\n",
      "epoch  18  Amari metric:  0.335768119233\n",
      "loss  [-278252.09375  -223579.109375]\n",
      "metrices  [ 8.02483273  5.63966751]\n",
      "dist  0.10344037066\n",
      "epoch  19  Amari metric:  0.267354518776\n",
      "loss  [ -261210.90625 -1709705.     ]\n",
      "metrices  [ 7.63066196  5.91534138]\n",
      "dist  0.0627535269546\n",
      "epoch  20  Amari metric:  0.209536299119\n",
      "loss  [ -301966.  -5923573.5]\n",
      "metrices  [ 7.16865873  4.76522398]\n",
      "dist  0.0547356453624\n",
      "epoch  21  Amari metric:  0.155890561524\n",
      "loss  [  -365682.75 -16425796.  ]\n",
      "metrices  [ 6.8068018   4.27812099]\n",
      "dist  0.0519386273863\n",
      "epoch  22  Amari metric:  0.16378700351\n",
      "loss  [  -461475.46875 -41458044.     ]\n",
      "metrices  [ 6.1195097  3.0055604]\n",
      "dist  0.00769857283186\n",
      "epoch  23  Amari metric:  0.121051859108\n",
      "loss  [  -445697.53125 -36223104.     ]\n",
      "metrices  [ 5.92061758  2.57015777]\n",
      "dist  0.04189831059\n",
      "epoch  24  Amari metric:  0.152989677786\n",
      "loss  [  -543199.625 -73806432.   ]\n",
      "metrices  [ 5.24913788  0.83068776]\n",
      "dist  0.0313528106914\n",
      "epoch  25  Amari metric:  0.114176615039\n",
      "loss  [  -470845.6875 -43724900.    ]\n",
      "metrices  [ 4.91113281  0.47631243]\n",
      "dist  0.0381416613599\n",
      "epoch  26  Amari metric:  0.151949924447\n",
      "loss  [  -564668.75 -82925448.  ]\n",
      "metrices  [ 4.46197462 -1.46713543]\n",
      "dist  0.0371242491758\n"
     ]
    }
   ],
   "source": [
    "#M=200 lbd=0.05还行\n",
    "#神经网络需要调整参数，不是很好的问题，不值得太花费时间.\n",
    "maxiters =50\n",
    "epochs =200\n",
    "batch_size =1024\n",
    "m  =2\n",
    "M  =200\n",
    "W =np.random.normal(size=(m,m))\n",
    "W =orth(W)\n",
    "W_last =np.random.normal(size=(m,m))\n",
    "print('Amari metric ',Amari_metric(target,W))\n",
    "models =[]\n",
    "inputs =[]\n",
    "outputs =[]\n",
    "loss     =np.zeros((maxiters,m),dtype=float)\n",
    "metrices =np.zeros((maxiters,m),dtype=float)\n",
    "y_pred =np.zeros((1024,m),dtype=float)\n",
    "tol =1e-7\n",
    "for i in range(m):\n",
    "    input_tensor =Input(shape=(1,),dtype='float32')\n",
    "    x =layers.Dense(M,activation='sigmoid',kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(input_tensor)\n",
    "    #x =layers.Dense(50,activation='sigmoid',kernel_initializer='random_uniform',\n",
    "    #            bias_initializer='zeros',kernel_regularizer=regularizers.l1(1))(x)\n",
    "    output_tensor =layers.Dense(1,activation=\"linear\",kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros',kernel_regularizer=regularizers.l2(0.09))(x)\n",
    "    model =Model(input_tensor,output_tensor)\n",
    "    model.compile(optimizer='rmsprop',loss=mdi_loss4,metrics=[negentropy])\n",
    "    models.append(model)\n",
    "    inputs.append(input_tensor)\n",
    "    outputs.append(output_tensor)\n",
    "\n",
    "for i in range(maxiters):\n",
    "    #X_train =(W@X.T).T\n",
    "    #y_train =gauss_standard(X_train)\n",
    "    Xs =(W@X.T).T\n",
    "    #[X_train,y_train] =group(X_train)\n",
    "    print('dist ',Amari_metric(W_last,W))\n",
    "    if(Amari_metric(W_last,W)<tol):\n",
    "        break\n",
    "    W_last =W.copy()\n",
    "    for j in range(m):\n",
    "        [X_train,y_train] =group(Xs[:,j],B=500)\n",
    "        y_train =y_train/gauss_standard(X_train)\n",
    "        for k in range(epochs):\n",
    "            [loss[i,j],metrices[i,j]] =models[j].train_on_batch(x=X_train,y=y_train)\n",
    "        #models[j].fit(X_train[:,j][:,np.newaxis],y_train[:,j][:,np.newaxis],epochs=100,batch_size=1024,verbose=0)\n",
    "        grad  =K.gradients(models[j].output,[models[j].input])[0]\n",
    "        grad2 =K.gradients(grad,[models[j].input])[0]\n",
    "        grads =K.function([models[j].input],[grad,grad2])\n",
    "        #print('grad ',grad)\n",
    "        #print('grad2 ',grad2)\n",
    "        #K.update(models[j].input,X_train[:,j])\n",
    "        [grad_val,grad2_val] =grads([Xs[:,j][:,np.newaxis]])\n",
    "        \n",
    "        #print('norm ',np.linalg.norm(grad_val))\n",
    "        #print('grad_val ',grad_val)\n",
    "        W[j,:] =np.mean(X*grad_val,axis=0)-np.mean(grad2_val)*W[j,:]\n",
    "        if(j>0):\n",
    "            W[j,:] =W[j,:]-(np.sum(W[j,:]*W[j-1,:]))*W[j-1,:]\n",
    "        W[j,:] =W[j,:]/np.linalg.norm(W[j,:])\n",
    "    #print('before W ',W)\n",
    "    #W =orth(W)\n",
    "    #if(i<10):\n",
    "    #    print('W_last ',W_last)\n",
    "    #    print('W ',W )\n",
    "    print('epoch ',i,' Amari metric: ',Amari_metric(target,W))\n",
    "    print('loss ',loss[i,:])\n",
    "    print('metrices ',metrices[i,:])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group(Xs[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0  begin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-77bd19db8272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneuralICA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mW0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlbd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-5e678bbb91cf>\u001b[0m in \u001b[0;36mneuralICA\u001b[0;34m(X, W0, M, maxiters, epochs, optimizer, tol, activation, lbd)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m#print('grad2 ',grad2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;31m#K.update(models[j].input,X_train[:,j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mgrad_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad2_val\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m#print('norm ',np.linalg.norm(grad_val))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 run_metadata):\n\u001b[1;32m   1016\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1066\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "[W,metrics]=neuralICA(X,W0=W0,M=200,maxiters=20,epochs=200,optimizer='rmsprop',tol=1e-7,activation='sigmoid',lbd=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 100, 1024)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition  1.37894287867\n",
      "condition  1.253542588\n",
      "condition  1.55775619345\n",
      "condition  1.45484727925\n",
      "condition  1.00702291659\n",
      "condition  1.0038730418\n",
      "condition  1.21191847819\n",
      "condition  1.0454329456\n",
      "condition  1.07785104355\n",
      "condition  1.24011916343\n",
      "condition  1.28180860331\n",
      "condition  1.03850530372\n",
      "condition  1.06004238406\n",
      "condition  1.09420849669\n",
      "condition  1.22337499737\n",
      "condition  1.25175975608\n",
      "condition  1.14557891996\n",
      "condition  1.64536132015\n",
      "condition  1.06905215078\n",
      "condition  1.07057263912\n",
      "condition  1.5233731116\n",
      "condition  1.29257158662\n",
      "condition  1.26812319581\n",
      "condition  1.11100397832\n",
      "condition  1.44701599948\n",
      "condition  1.42649475442\n",
      "condition  1.20707971668\n",
      "condition  1.34022959757\n",
      "condition  1.42477909621\n",
      "condition  1.36521509776\n",
      "condition  1.03017156495\n",
      "condition  1.53077594929\n",
      "condition  1.05494480051\n",
      "condition  1.08987274595\n",
      "condition  1.15420903715\n",
      "condition  1.40239418926\n",
      "condition  1.03946812433\n",
      "condition  1.52994770336\n",
      "condition  1.2295761528\n",
      "condition  1.10868435732\n",
      "condition  1.40274415617\n",
      "condition  1.06388535603\n",
      "condition  1.01270304975\n",
      "condition  1.33831702414\n",
      "condition  1.04897237865\n",
      "condition  1.03160101348\n",
      "condition  1.83977653225\n",
      "condition  1.30813632443\n",
      "condition  1.3598586148\n",
      "condition  1.12261571974\n",
      "condition  1.17682300981\n",
      "condition  1.15529634994\n",
      "condition  1.65095651177\n",
      "condition  1.1651765743\n",
      "condition  1.05013145432\n",
      "condition  1.44765275944\n",
      "condition  1.07954542969\n",
      "condition  1.39958066524\n",
      "condition  1.51923323063\n",
      "condition  1.30543202562\n",
      "condition  1.09020163176\n",
      "condition  1.02460517576\n",
      "condition  1.65707480441\n",
      "condition  1.41487767164\n",
      "condition  1.08753413851\n",
      "condition  1.32962892055\n",
      "condition  1.25256785667\n",
      "condition  1.00377611303\n",
      "condition  1.48640884824\n",
      "condition  1.04498289031\n",
      "condition  1.0682364075\n",
      "condition  1.24726215245\n",
      "condition  1.22143465874\n",
      "condition  1.114449043\n",
      "condition  1.0725174083\n",
      "condition  1.64102598093\n",
      "condition  1.48323136503\n",
      "condition  1.45593011514\n",
      "condition  1.22127040813\n",
      "condition  1.1195078722\n",
      "condition  1.38332140197\n",
      "condition  1.51980552625\n",
      "condition  1.27337937084\n",
      "condition  1.3132978778\n",
      "condition  1.09293909989\n",
      "condition  1.70385039976\n",
      "condition  1.62482106003\n",
      "condition  1.00347528584\n",
      "condition  1.35502775037\n",
      "condition  1.00802293489\n",
      "condition  1.1018427544\n",
      "condition  1.50713264494\n",
      "condition  1.40680291654\n",
      "condition  1.43786256206\n",
      "condition  1.44988358336\n",
      "condition  1.00268037496\n",
      "condition  1.62319203737\n",
      "condition  1.7423946008\n",
      "condition  1.79218455188\n",
      "condition  1.02861103711\n",
      "condition  1.06776786753\n",
      "condition  1.30254350232\n",
      "condition  1.08983974074\n",
      "condition  1.23737058157\n",
      "condition  1.0294663637\n",
      "condition  1.82359747517\n",
      "condition  1.48572889923\n",
      "condition  1.23723847242\n",
      "condition  1.0576092218\n",
      "condition  1.46927307553\n",
      "condition  1.14514858453\n",
      "condition  1.30207994029\n",
      "condition  1.05112918142\n"
     ]
    }
   ],
   "source": [
    "dim2data =np.zeros((18,30,2,1024),dtype=float)\n",
    "dim2target =np.zeros((18,30,2,2),dtype=float)\n",
    "dim2W0 =np.zeros((18,30,2,2),dtype=float)\n",
    "dim2W  =np.zeros((18,30,2,2),dtype=float)\n",
    "amaris =np.zeros((18,30),dtype=float)\n",
    "maxiters =20\n",
    "epochs =20\n",
    "M=1024\n",
    "for ii in range(18):\n",
    "    for jj in range(30):\n",
    "        m  =2\n",
    "        S  =data[ii,[jj,jj+30],:].T\n",
    "        A0 =mixmat(2)\n",
    "        X  =(A0@S.T).T\n",
    "        X  =scale(X,center=True,scale=False)\n",
    "        [P,X] =whiten(X)\n",
    "        target =np.linalg.inv(P@A0)\n",
    "        W0 =np.random.normal(size=(2,2))\n",
    "        W0 =orth(W0)\n",
    "        \n",
    "        W =W0.copy()\n",
    "        W_last =np.random.normal(size=(m,m))\n",
    "\n",
    "        models =[]\n",
    "        inputs =[]\n",
    "        outputs =[]\n",
    "        loss     =np.zeros((maxiters,m),dtype=float)\n",
    "        metrices =np.zeros((maxiters,m),dtype=float)\n",
    "        y_pred =np.zeros((1024,m),dtype=float)\n",
    "        tol =1e-7\n",
    "        for i in range(m):\n",
    "            input_tensor =Input(shape=(1,),dtype='float32')\n",
    "            x =layers.Dense(M,activation='sigmoid',kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')(input_tensor)\n",
    "            output_tensor =layers.Dense(1,activation=\"linear\",kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros',kernel_regularizer=regularizers.l2(0.05))(x)\n",
    "            model =Model(input_tensor,output_tensor)\n",
    "            model.compile(optimizer='rmsprop',loss=mdi_loss2,metrics=[negentropy])\n",
    "            models.append(model)\n",
    "            inputs.append(input_tensor)\n",
    "            outputs.append(output_tensor)\n",
    "        for i in range(maxiters):\n",
    "            \n",
    "            X_train =(W@X.T).T\n",
    "            []\n",
    "            if(Amari_metric(W_last,W)<tol):\n",
    "                break\n",
    "            W_last =W.copy()\n",
    "            for j in range(m):\n",
    "                for k in range(epochs):\n",
    "                    [loss[i,j],metrices[i,j]] =models[j].train_on_batch(x=X_train[:,j],y=y_train[:,j])\n",
    "                grad  =K.gradients(models[j].output,[models[j].input])[0]\n",
    "                grad2 =K.gradients(grad,[models[j].input])[0]\n",
    "                grads =K.function([models[j].input],[grad,grad2])\n",
    "                [grad_val,grad2_val] =grads([X_train[:,j][:,np.newaxis]])\n",
    "                W[j,:] =np.mean(X*grad_val,axis=0)-np.mean(grad2_val)*W[j,:]\n",
    "\n",
    "            W =orth(W)\n",
    "            #print('epoch ',i,' Amari metric: ',Amari_metric(target,W))\n",
    "            #print('loss ',loss[i,:])\n",
    "            #print('metrices ',metrices[i,:])\n",
    "        \n",
    "    dim2data[ii,jj,:,:]=S.T.copy()\n",
    "    dim2target[ii,jj,:,:] =target.copy()\n",
    "    dim2W0[ii,jj,:,:]=W0.copy()\n",
    "    dim2W[ii,jj,:,:] =W.copy()\n",
    "    amaris[ii,jj] =Amari_metric(target,W)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amari metric  0.312182193261\n",
      "dist  0.725816088103\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation 'gradients_818/dense_112/Softplus_grad/SoftplusGrad' (op type: SoftplusGrad)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m               \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_gradient_function\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1643\u001b[0m     \u001b[0mop_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/framework/registry.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     92\u001b[0m       raise LookupError(\n\u001b[0;32m---> 93\u001b[0;31m           \"%s registry has no entry for: %s\" % (self._name, name))\n\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m: gradient registry has no entry for: SoftplusGrad",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ebcf2c81d682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#models[j].fit(X_train[:,j][:,np.newaxis],y_train[:,j][:,np.newaxis],epochs=100,batch_size=1024,verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mgrad\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mgrad2\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m#print('grad ',grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2392\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \"\"\"\n\u001b[0;32m-> 2394\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)\u001b[0m\n\u001b[1;32m    532\u001b[0m               raise LookupError(\n\u001b[1;32m    533\u001b[0m                   \u001b[0;34m\"No gradient defined for operation '%s' (op type: %s)\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m                   (op.name, op.type))\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloop_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m           \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnterGradWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: No gradient defined for operation 'gradients_818/dense_112/Softplus_grad/SoftplusGrad' (op type: SoftplusGrad)"
     ]
    }
   ],
   "source": [
    "epochs =20\n",
    "batch_size =1024\n",
    "m  =2\n",
    "M  =50\n",
    "W =np.random.normal(size=(m,m))\n",
    "W =orth(W)\n",
    "W_last =np.random.normal(size=(m,m))\n",
    "print('Amari metric ',Amari_metric(target,W))\n",
    "models =[]\n",
    "inputs =[]\n",
    "outputs =[]\n",
    "loss     =np.zeros((epochs,m),dtype=float)\n",
    "metrices =np.zeros((epochs,m),dtype=float)\n",
    "y_pred =np.zeros((1024,m),dtype=float)\n",
    "tol =1e-7\n",
    "for i in range(m):\n",
    "    input_tensor =Input(shape=(1,),dtype='float32')\n",
    "    x =layers.Dense(M,activation='sigmoid',kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros',kernel_regularizer=regularizers.l2(0.001))(input_tensor)\n",
    "    x =layers.Dense(50,activation='sigmoid',kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros',kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    output_tensor =layers.Dense(1,activation=\"softplus\",kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros',kernel_regularizer=regularizers.l2(0.05))(x)\n",
    "    model =Model(input_tensor,output_tensor)\n",
    "    model.compile(optimizer='rmsprop',loss=mdi_loss5,metrics=[negentropy])\n",
    "    models.append(model)\n",
    "    inputs.append(input_tensor)\n",
    "    outputs.append(output_tensor)\n",
    "\n",
    "for i in range(epochs):\n",
    "    #X_train =(W@X.T).T\n",
    "    #y_train =gauss_standard(X_train)\n",
    "    Xs =(W@X.T).T\n",
    "    #[X_train,y_train] =group(X_train)\n",
    "    print('dist ',Amari_metric(W_last,W))\n",
    "    if(Amari_metric(W_last,W)<tol):\n",
    "        break\n",
    "    W_last =W.copy()\n",
    "    for j in range(m):\n",
    "        [X_train,y_train] =group(Xs[:,j],B=500)\n",
    "        \n",
    "        for k in range(400):\n",
    "            [loss[i,j],metrices[i,j]] =models[j].train_on_batch(x=X_train,y=y_train)\n",
    "        #models[j].fit(X_train[:,j][:,np.newaxis],y_train[:,j][:,np.newaxis],epochs=100,batch_size=1024,verbose=0)\n",
    "        grad  =K.gradients(models[j].output,[models[j].input])[0]\n",
    "        grad2 =K.gradients(grad,[models[j].input])[0]\n",
    "        grads =K.function([models[j].input],[grad,grad2,models[j].output])\n",
    "        #print('grad ',grad)\n",
    "        #print('grad2 ',grad2)\n",
    "        #K.update(models[j].input,X_train[:,j])\n",
    "        [grad_val,grad2_val,y_pred] =grads([Xs[:,j][:,np.newaxis]])\n",
    "        g1 =grad_val/y_pred\n",
    "        g2 =(grad2_val*y_pred-grad_val*grad_val)/(y_pred*y_pred)\n",
    "        \n",
    "        #print('norm ',np.linalg.norm(grad_val))\n",
    "        #print('grad_val ',grad_val)\n",
    "        #W[j,:] =W[j,:] -(np.mean(X*g1,axis=0))/np.mean(g2)\n",
    "        W[j,:] =np.mean(X*g1,axis=0)-np.mean(g2)*W[j,:]\n",
    "        #if(j>0):\n",
    "        #    W[j,:] =W[j,:]-(np.sum(W[j,:]*W[j-1,:]))*W[j-1,:]\n",
    "        #W[j,:] =W[j,:]/np.linalg.norm(W[j,:])\n",
    "    #print('before W ',W)\n",
    "    W =orth(W)\n",
    "    #if(i<10):\n",
    "    #    print('W_last ',W_last)\n",
    "    #    print('W ',W )\n",
    "    print('epoch ',i,' Amari metric: ',Amari_metric(target,W))\n",
    "    print('loss ',loss[i,:])\n",
    "    print('metrices ',metrices[i,:])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
